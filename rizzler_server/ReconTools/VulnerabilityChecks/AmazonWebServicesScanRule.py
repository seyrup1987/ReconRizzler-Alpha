# Filename: VulnerabilityChecks/AmazonWebServicesScanRule.py

import asyncio
import logging
import random
import string
import uuid
import urllib.parse
import xml.etree.ElementTree as ET # For parsing S3 list responses
import json # For ES/OpenSearch responses
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple, Set

import aiohttp

# --- Alert Dataclass ---
@dataclass
class AWSVulnerabilityAlert:
    # Non-default arguments first
    cwe_id: int
    name: str
    description: str
    uri: str
    
    # Default arguments follow
    wasc_id: Optional[int] = None
    attack_details: Dict[str, Any] = field(default_factory=dict)
    evidence: str = ""
    solution: str = (
        "Review AWS service configurations against security best practices (e.g., AWS Well-Architected Framework, CIS Benchmarks). "
        "Apply principle of least privilege. Use IAM roles, security groups, NACLs, and VPC endpoints effectively. "
        "Regularly audit public exposure and permissions."
    )
    reference: str = ""
    risk: int = 3
    confidence: int = 2
    alert_tags: Dict[str, str] = field(default_factory=dict)

    def as_dict(self) -> Dict[str, Any]:
        return {
            "vulnerability_type": self.alert_tags.get("Service", "AWS_Misconfiguration"),
            "type": self.name,
            "detail": self.description,
            "payload_used": str(self.attack_details.get("payload", self.attack_details.get("filename", "N/A"))),
            "injection_point_info": f"{self.attack_details.get('check_type', 'N/A')}: {self.attack_details.get('path', self.attack_details.get('port', 'N/A'))}",
            "url_tested": self.uri,
            "severity": {1: "Info", 2: "Low", 3: "Medium", 4: "High"}.get(self.risk, "Unknown"),
            "confidence": {1: "Low", 2: "Medium", 3: "High"}.get(self.confidence, "Unknown"),
            "evidence": self.evidence,
            "solution": self.solution,
            "reference": self.reference,
            "cwe_id": self.cwe_id,
            "wasc_id": self.wasc_id if self.wasc_id else 0,
            "alert_tags": self.alert_tags,
            "other_info": f"Attack Details: {self.attack_details}"
        }

# --- Main Scan Rule Class ---
class AmazonWebServicesScanRule:
    tech_required: Set[str] = {"aws", "amazon web services"} # Broad trigger

    S3_COMMON_SENSITIVE_FILES = [
        ".env", ".env.prod", ".env.production", "credentials", "config.json", "settings.json",
        "backup.zip", "backup.tar.gz", "site.zip", "db_backup.sql", ".git/config",
        "id_rsa", "id_dsa", ".ssh/authorized_keys", "secret_key.txt", "aws_keys.csv"
    ]
    S3_WRITE_TEST_FILENAME_TEMPLATE = "aws_scan_write_test_{random_hex}.txt"
    S3_WRITE_TEST_CONTENT = "AWS_SCANNER_WRITE_TEST_SUCCESSFUL"

    ES_OPENSEARCH_DEFAULT_PORTS = [9200, 9201] # HTTP, sometimes HTTPS on 9200 too
    REDSHIFT_DEFAULT_PORT = 5439
    RDS_MYSQL_DEFAULT_PORT = 3306
    RDS_POSTGRES_DEFAULT_PORT = 5432
    ELASTICACHE_REDIS_DEFAULT_PORT = 6379
    ELASTICACHE_MEMCACHED_DEFAULT_PORT = 11211

    IMDSV1_BASE_URL = "http://169.254.169.254/latest/meta-data/"
    IMDSV2_TOKEN_URL = "http://169.254.169.254/latest/api/token"
    IMDS_COMMON_PATHS = [
        "iam/security-credentials/", # Needs role name appended
        "instance-id",
        "hostname",
        "public-ipv4",
        "local-ipv4",
        "security-groups",
        "user-data"
    ]


    def __init__(self,
                 parent_logger: Optional[logging.Logger] = None,
                 oast_base_url: Optional[str] = None, # For potential OAST confirmation in S3 write
                 http_timeout: float = 10.0, # Shorter for AWS service checks
                 tcp_connect_timeout: float = 3.0, # For port checks
                 active_scan_async_q: Optional[asyncio.Queue] = None
                 ):
        self.logger = parent_logger or logging.getLogger(__name__)
        self.oast_base_url = self._normalize_oast_url(oast_base_url) if oast_base_url else None
        self.http_timeout = http_timeout
        self.tcp_connect_timeout = tcp_connect_timeout
        self.active_scan_async_q = active_scan_async_q

    def _normalize_oast_url(self, url: str) -> Optional[str]:
        if not url: return None
        parsed = urllib.parse.urlparse(url)
        return parsed.netloc if parsed.netloc else None

    async def _send_request(
        self, session: aiohttp.ClientSession, method: str, url: str,
        headers: Optional[Dict[str, str]] = None, params: Optional[Dict[str, Any]] = None,
        data: Optional[Any] = None, allow_redirects: bool = True
    ) -> Tuple[Optional[aiohttp.ClientResponse], str, Dict[str, str], Optional[bytes]]:
        req_headers = (headers or {}).copy()
        if "User-Agent" not in req_headers:
            req_headers["User-Agent"] = f"AWSScanner/1.0 ({uuid.uuid4().hex[:8]})"

        self.logger.debug(f"AWSScan: {method} {url}, Headers: {req_headers.keys()}, Data: {str(data)[:100] if data else 'None'}")
        resp_text, resp_headers_dict, resp_bytes = "", {}, None
        resp_obj = None
        try:
            timeout = aiohttp.ClientTimeout(total=self.http_timeout)
            async with session.request(
                method.upper(), url, headers=req_headers, params=params, data=data,
                timeout=timeout, allow_redirects=allow_redirects
            ) as resp:
                resp_obj = resp
                try:
                    resp_bytes = await resp.read()
                    resp_text = resp_bytes.decode(errors='replace')
                except Exception as e_read:
                    self.logger.warning(f"Error reading response from {url}: {e_read}")
                resp_headers_dict = dict(resp.headers)
                self.logger.debug(f"AWSScan: Response from {url}: Status {resp.status}")
        except asyncio.TimeoutError:
            self.logger.warning(f"AWSScan: Request to {url} timed out.")
        except aiohttp.ClientError as e:
            self.logger.warning(f"AWSScan: Request to {url} client error: {e}")
        except Exception as e_req:
            self.logger.error(f"AWSScan: Unexpected error for {url}: {e_req}")
        return resp_obj, resp_text, resp_headers_dict, resp_bytes

    async def _emit_alert(self, alert_obj: AWSVulnerabilityAlert):
        if self.active_scan_async_q:
            try:
                await self.active_scan_async_q.put({
                    "type": "active_scan_alert", "alert": alert_obj.as_dict()
                })
                self.logger.info(f"AWSScan: Emitted alert - {alert_obj.name} for {alert_obj.uri}")
            except Exception as e:
                self.logger.error(f"AWSScan: Failed to emit alert to queue: {e}")
        else:
            self.logger.warning(f"AWSScan: Async queue not available. Alert not emitted: {alert_obj.name}")

    async def _check_tcp_port_open(self, host: str, port: int) -> bool:
        try:
            self.logger.debug(f"AWSScan: TCP Check - Attempting connection to {host}:{port}")
            fut = asyncio.open_connection(host, port)
            reader, writer = await asyncio.wait_for(fut, timeout=self.tcp_connect_timeout)
            writer.close()
            await writer.wait_closed()
            self.logger.debug(f"AWSScan: TCP Check - Connection to {host}:{port} successful.")
            return True
        except (ConnectionRefusedError, asyncio.TimeoutError, OSError) as e:
            self.logger.debug(f"AWSScan: TCP Check - Connection to {host}:{port} failed: {type(e).__name__}")
            return False
        except Exception as e_generic:
            self.logger.warning(f"AWSScan: TCP Check - Unexpected error connecting to {host}:{port}: {e_generic}")
            return False

    # --- S3 Checks ---
    async def _check_s3_publicly_listable(self, session: aiohttp.ClientSession, bucket_url: str):
        self.logger.debug(f"AWS S3: Checking if bucket {bucket_url} is publicly listable.")
        resp_obj, resp_text, _, _ = await self._send_request(session, "GET", bucket_url)

        if resp_obj and resp_obj.status == 200:
            try:
                # S3 list response is XML
                root = ET.fromstring(resp_text)
                if root.tag.endswith("ListBucketResult") and root.findtext("{*}Name") == urllib.parse.urlparse(bucket_url).hostname.split('.')[0]:
                    num_keys = len(root.findall("{*}Contents"))
                    alert = AWSVulnerabilityAlert(
                        cwe_id=538, wasc_id=13, # File and Directory Information Exposure
                        name="Amazon S3 Bucket Publicly Listable",
                        description=f"The S3 bucket '{bucket_url}' allows anonymous listing of its contents.",
                        uri=bucket_url,
                        attack_details={"check_type": "S3 Listable", "method": "GET"},
                        evidence=f"HTTP 200 and XML ListBucketResult received. Found {num_keys} objects in listing.",
                        reference="https://docs.aws.amazon.com/AmazonS3/latest/userguide/block-public-access.html",
                        risk=3, confidence=3,
                        alert_tags={"Service": "S3", "Misconfiguration": "PublicList"}
                    )
                    await self._emit_alert(alert)
            except ET.ParseError:
                self.logger.debug(f"AWS S3: Response from {bucket_url} was not valid XML for listing.")
            except Exception as e_xml:
                 self.logger.warning(f"AWS S3: Error parsing XML for {bucket_url}: {e_xml}")


    async def _check_s3_publicly_writable(self, session: aiohttp.ClientSession, bucket_url: str):
        random_hex = uuid.uuid4().hex[:8]
        test_filename = self.S3_WRITE_TEST_FILENAME_TEMPLATE.format(random_hex=random_hex)
        test_file_url = urllib.parse.urljoin(bucket_url, test_filename)
        test_content = f"{self.S3_WRITE_TEST_CONTENT}_{random_hex}".encode('utf-8')

        self.logger.debug(f"AWS S3: Attempting to PUT test file to {test_file_url}")
        put_headers = {"Content-Type": "text/plain"}
        resp_obj_put, _, _, _ = await self._send_request(session, "PUT", test_file_url, headers=put_headers, data=test_content)

        if resp_obj_put and resp_obj_put.status in [200, 201]: # OK or Created
            self.logger.info(f"AWS S3: PUT to {test_file_url} successful (Status: {resp_obj_put.status}). Verifying content...")
            
            # Verify by GETting the uploaded file
            await asyncio.sleep(0.2) # S3 eventual consistency
            resp_obj_get, _, _, resp_bytes_get = await self._send_request(session, "GET", test_file_url)

            writable_confirmed = False
            evidence_str = f"PUT successful (HTTP {resp_obj_put.status}). "
            if resp_obj_get and resp_obj_get.status == 200 and resp_bytes_get == test_content:
                writable_confirmed = True
                evidence_str += f"GET successful and content matches. File: {test_file_url}"
            
            # Cleanup: Attempt to DELETE the uploaded file
            self.logger.debug(f"AWS S3: Attempting to DELETE {test_file_url}")
            await self._send_request(session, "DELETE", test_file_url)

            if writable_confirmed:
                alert = AWSVulnerabilityAlert(
                    cwe_id=284, wasc_id=1, # Improper Access Control / Insufficient Authorization
                    name="Amazon S3 Bucket Publicly Writable",
                    description=f"The S3 bucket '{bucket_url}' allows anonymous users to upload/overwrite files.",
                    uri=bucket_url,
                    attack_details={"check_type": "S3 Writable", "method": "PUT/GET/DELETE", "filename": test_filename},
                    evidence=evidence_str,
                    reference="https://docs.aws.amazon.com/AmazonS3/latest/userguide/block-public-access.html",
                    risk=4, confidence=3,
                    alert_tags={"Service": "S3", "Misconfiguration": "PublicWrite"}
                )
                await self._emit_alert(alert)
        else:
            self.logger.debug(f"AWS S3: PUT to {test_file_url} failed or was not accepted (Status: {resp_obj_put.status if resp_obj_put else 'N/A'}).")

    async def _check_s3_common_sensitive_files(self, session: aiohttp.ClientSession, bucket_url: str):
        for filename in self.S3_COMMON_SENSITIVE_FILES:
            target_url = urllib.parse.urljoin(bucket_url, filename)
            self.logger.debug(f"AWS S3: Checking for sensitive file {target_url}")
            resp_obj, resp_text, _, _ = await self._send_request(session, "GET", target_url)

            if resp_obj and resp_obj.status == 200 and len(resp_text) > 0: # Check for non-empty response
                # Avoid flagging generic XML error pages from S3 if file not found but bucket is public
                if not (resp_text.strip().startswith("<?xml") and "<Error>" in resp_text and "<Code>NoSuchKey</Code>" in resp_text):
                    alert = AWSVulnerabilityAlert(
                        cwe_id=538, wasc_id=13, # File and Directory Information Exposure
                        name=f"Amazon S3 Bucket Exposes Potentially Sensitive File: {filename}",
                        description=f"A potentially sensitive file '{filename}' was found publicly accessible in bucket '{bucket_url}'.",
                        uri=target_url,
                        attack_details={"check_type": "S3 Sensitive File", "filename": filename, "method": "GET"},
                        evidence=f"HTTP 200 for {target_url}. Snippet: {resp_text[:100]}",
                        reference="https.aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/",
                        risk=3, confidence=2, # Confidence medium as content needs manual review
                        alert_tags={"Service": "S3", "Misconfiguration": "SensitiveFile"}
                    )
                    await self._emit_alert(alert)

    # --- Elasticsearch/OpenSearch Checks ---
    async def _check_unauthenticated_es_opensearch(self, session: aiohttp.ClientSession, es_base_url: str):
        # Try common API endpoints that indicate an open cluster
        endpoints_to_check = ["/", "/_cat/indices", "/_nodes", "/_cluster/health"]
        for endpoint_path in endpoints_to_check:
            target_url = urllib.parse.urljoin(es_base_url, endpoint_path.lstrip('/'))
            self.logger.debug(f"AWS ES/OpenSearch: Checking unauthenticated access to {target_url}")
            resp_obj, resp_text, _, _ = await self._send_request(session, "GET", target_url)

            if resp_obj and resp_obj.status == 200:
                try:
                    json_data = json.loads(resp_text)
                    # Check for characteristic keys
                    if ("version" in json_data and "number" in json_data["version"]) or \
                       (isinstance(json_data, list) and endpoint_path == "/_cat/indices") or \
                       ("cluster_name" in json_data):
                        alert = AWSVulnerabilityAlert(
                            cwe_id=284, wasc_id=1, # Improper Access Control
                            name="Amazon Elasticsearch/OpenSearch Service Unauthenticated Access",
                            description=f"The Elasticsearch/OpenSearch service at '{es_base_url}' allows unauthenticated access to API endpoint '{endpoint_path}'.",
                            uri=target_url,
                            attack_details={"check_type": "ES/OpenSearch Unauth", "path": endpoint_path, "method": "GET"},
                            evidence=f"HTTP 200 and valid JSON response from {target_url}. Snippet: {resp_text[:150]}",
                            reference="https://docs.aws.amazon.com/opensearch-service/latest/developerguide/fgac.html",
                            risk=4, confidence=3,
                            alert_tags={"Service": "Elasticsearch/OpenSearch", "Misconfiguration": "UnauthenticatedAccess"}
                        )
                        await self._emit_alert(alert)
                        return # Found one open endpoint, that's enough for this check
                except json.JSONDecodeError:
                    self.logger.debug(f"AWS ES/OpenSearch: Response from {target_url} was not valid JSON.")
                except Exception as e_json:
                    self.logger.warning(f"AWS ES/OpenSearch: Error processing JSON for {target_url}: {e_json}")


    # --- Generic Port Exposure Checks ---
    async def _check_exposed_service_port(self, host: str, port: int, service_name: str, cve_ref: Optional[str] = None):
        if await self._check_tcp_port_open(host, port):
            cwe = 200 # Information Exposure (that port is open)
            risk = 2 # Low to Medium depending on service
            desc_cve_part = f" (potentially related to {cve_ref})" if cve_ref else ""
            ref_cve_part = f", {cve_ref}" if cve_ref else ""

            alert = AWSVulnerabilityAlert(
                cwe_id=cwe,
                name=f"AWS Service Port Exposed: {service_name} on port {port}",
                description=f"The default port {port} for {service_name} is publicly accessible on host '{host}'{desc_cve_part}. This could indicate an overly permissive security group or NACL.",
                uri=f"{host}:{port}",
                attack_details={"check_type": "Port Exposure", "service": service_name, "port": port},
                evidence=f"TCP connection to {host}:{port} was successful.",
                reference=f"Review AWS Security Group and NACL configurations for {service_name}{ref_cve_part}.",
                risk=risk, confidence=3,
                alert_tags={"Service": service_name, "Misconfiguration": "PortExposed", "CVE_Hint": cve_ref if cve_ref else "N/A"}
            )
            await self._emit_alert(alert)

    # --- IMDS Helper ---
    def get_imds_ssrf_payloads(self, target_ssrf_param_url_template: str) -> List[Tuple[str, str]]:
        """
        Generates a list of full URLs to test for IMDS access via a known SSRF.
        Args:
            target_ssrf_param_url_template: A URL template for the SSRF, e.g., "http://vuln-app.com/ssrf?url={SSRF_TARGET}"
                                            where {SSRF_TARGET} will be replaced.
        Returns:
            List of (IMDS Path Description, Full SSRF URL to try)
        """
        payloads = []
        # IMDSv1
        for path in self.IMDS_COMMON_PATHS:
            imds_url = urllib.parse.urljoin(self.IMDSV1_BASE_URL, path)
            payloads.append((f"IMDSv1 - {path}", target_ssrf_param_url_template.format(SSRF_TARGET=imds_url)))
        
        # For IMDSv2, one would first need to get a token, then use it.
        # This is a two-step process via SSRF, more complex.
        # For now, just list the token URL. A sophisticated SSRF tool would handle the two stages.
        payloads.append(("IMDSv2 - Token Request", target_ssrf_param_url_template.format(SSRF_TARGET=self.IMDSV2_TOKEN_URL)))
        return payloads

    # --- Main Scan Orchestration Method ---
    async def scan(
        self,
        base_app_url: str, # The primary URL/endpoint being scanned
        aiohttp_session: aiohttp.ClientSession,
        service_type_hint: Optional[str] = None, # e.g., "s3", "ec2", "es", "redshift_endpoint", "generic_host"
        # Other args for compatibility with ActiveVulnerabilityCheck, may not all be used by this rule
        original_url: Optional[str] = None,
        original_headers: Optional[Dict[str, str]] = None,
        original_cookies: Optional[Dict[str, str]] = None,
        injection_details: Optional[Dict[str, Any]] = None,
        html_content: Optional[str] = None
    ):
        self.logger.info(f"AmazonWebServicesScanRule: Starting scan for target='{base_app_url}', service_hint='{service_type_hint}'")

        parsed_url = urllib.parse.urlparse(base_app_url)
        host = parsed_url.hostname
        if not host:
            self.logger.error(f"AWSScan: Could not determine hostname from base_app_url: {base_app_url}")
            return

        # --- S3 Bucket Checks ---
        if service_type_hint == "s3" or ".s3.amazonaws.com" in host or ".s3-website" in host or host.startswith("s3."):
            # Ensure base_app_url is the root of the bucket for S3 checks
            s3_bucket_url_root = f"{parsed_url.scheme}://{parsed_url.netloc}/"
            await self._check_s3_publicly_listable(aiohttp_session, s3_bucket_url_root)
            await self._check_s3_publicly_writable(aiohttp_session, s3_bucket_url_root)
            await self._check_s3_common_sensitive_files(aiohttp_session, s3_bucket_url_root)

        # --- Elasticsearch/OpenSearch Checks ---
        # Hint could be "es", "opensearch", or URL might contain ".es.amazonaws.com"
        elif service_type_hint in ["es", "opensearch"] or ".es.amazonaws.com" in host or ".opensearchservice.amazonaws.com" in host:
            es_url_to_check = base_app_url # Assume base_app_url is the ES endpoint
            # If not HTTPS, also try default HTTP port if base_app_url is just a hostname
            if parsed_url.scheme != "https" and not parsed_url.port:
                 for port in self.ES_OPENSEARCH_DEFAULT_PORTS:
                     http_es_url = f"http://{host}:{port}/"
                     await self._check_unauthenticated_es_opensearch(aiohttp_session, http_es_url)
            await self._check_unauthenticated_es_opensearch(aiohttp_session, es_url_to_check)


        # --- Port Exposure Checks (Redshift, RDS, ElastiCache) ---
        # These are typically run if the service_type_hint is "ec2", "generic_host",
        # or a specific DB endpoint hint.
        elif service_type_hint == "redshift_endpoint" or ".redshift.amazonaws.com" in host:
            await self._check_exposed_service_port(host, self.REDSHIFT_DEFAULT_PORT, "Amazon Redshift", "CVE-2025-5279 (Exposure)")
        
        elif service_type_hint == "rds_mysql_endpoint" or ".rds.amazonaws.com" in host: # Generic RDS, try common DBs
            await self._check_exposed_service_port(host, self.RDS_MYSQL_DEFAULT_PORT, "Amazon RDS (MySQL)")
        
        elif service_type_hint == "rds_postgres_endpoint" or ".rds.amazonaws.com" in host:
            await self._check_exposed_service_port(host, self.RDS_POSTGRES_DEFAULT_PORT, "Amazon RDS (PostgreSQL)")

        elif service_type_hint == "elasticache_redis_endpoint" or ".cache.amazonaws.com" in host:
            await self._check_exposed_service_port(host, self.ELASTICACHE_REDIS_DEFAULT_PORT, "Amazon ElastiCache (Redis)")

        elif service_type_hint == "elasticache_memcached_endpoint" or ".cache.amazonaws.com" in host:
            await self._check_exposed_service_port(host, self.ELASTICACHE_MEMCACHED_DEFAULT_PORT, "Amazon ElastiCache (Memcached)")
        
        elif service_type_hint in ["ec2", "generic_host"]: # If it's a generic EC2 or host, try common DB/cache ports
            self.logger.info(f"AWSScan: Probing common DB/Cache ports on generic host {host}")
            await self._check_exposed_service_port(host, self.REDSHIFT_DEFAULT_PORT, "Amazon Redshift", "CVE-2025-5279 (Exposure)")
            await self._check_exposed_service_port(host, self.RDS_MYSQL_DEFAULT_PORT, "Amazon RDS (MySQL)")
            await self._check_exposed_service_port(host, self.RDS_POSTGRES_DEFAULT_PORT, "Amazon RDS (PostgreSQL)")
            await self._check_exposed_service_port(host, self.ELASTICACHE_REDIS_DEFAULT_PORT, "Amazon ElastiCache (Redis)")
            await self._check_exposed_service_port(host, self.ELASTICACHE_MEMCACHED_DEFAULT_PORT, "Amazon ElastiCache (Memcached)")
            # Could also try ES/OpenSearch default HTTP ports here if not explicitly an ES endpoint
            for port in self.ES_OPENSEARCH_DEFAULT_PORTS:
                http_es_url = f"http://{host}:{port}/"
                await self._check_unauthenticated_es_opensearch(aiohttp_session, http_es_url)


        # Note: IMDS checks are not directly run here. The `get_imds_ssrf_payloads` method
        # would be called by a generic SSRF rule if it detects an SSRF on an AWS host.

        self.logger.info(f"AmazonWebServicesScanRule: Scan finished for target='{base_app_url}'")